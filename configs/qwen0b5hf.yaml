model:
    __type__: configs.Transformer_Config
    classname: qwen2.modeling_qwen2.Qwen2ForCausalLM
    tmix: qwen2rwkv
    hf_cfg:
        "attention_dropout": 0.0
        "bos_token_id": 151643
        "eos_token_id": 151643
        "hidden_act": "silu"
        "hidden_size": 896
        "initializer_range": 0.02
        "intermediate_size": 4864
        "max_position_embeddings": 131072
        "max_window_layers": 24
        "model_type": "qwen2"
        "num_attention_heads": 14
        "num_hidden_layers": 24
        "num_key_value_heads": 2
        "rms_norm_eps": 1e-06
        "rope_theta": 1000000.0
        "sliding_window": 131072
        "tie_word_embeddings": True
        "torch_dtype": "bfloat16"
        "transformers_version": "4.40.1"
        "use_cache": False, #True,
        "use_sliding_window": False
        "vocab_size": 151936
    ctx_len: 4096
    vocab_size: 151936
    vocab_padding_idx: 151646 
    num_key_value_heads: 2
    n_embd: 896
    dim_ffn: 4864
    n_layer: 24
    rope:
        __type__: configs.RoPE_Config
        base: 1000000.0

train:
    teacher:
        path: out/qwen2-0B5.safetensors
        model:
            __type__: configs.Transformer_Config
            classname: qwen2.modeling_qwen2.Qwen2ForCausalLM
            hf_cfg:
                "attention_dropout": 0.0
                "bos_token_id": 151643
                "eos_token_id": 151643
                "hidden_act": "silu"
                "hidden_size": 896
                "initializer_range": 0.02
                "intermediate_size": 4864
                "max_position_embeddings": 131072
                "max_window_layers": 24
                "model_type": "qwen2"
                "num_attention_heads": 14
                "num_hidden_layers": 24
                "num_key_value_heads": 2
                "rms_norm_eps": 1e-06
                "rope_theta": 1000000.0
                "sliding_window": 131072
                "tie_word_embeddings": True
                "torch_dtype": "bfloat16"
                "transformers_version": "4.40.1"
                "use_cache": False, #True,
                "use_sliding_window": False
                "vocab_size": 151936
            #tmix: qwen2
            classname: qwen2
            ctx_len: 4096
            vocab_size: 151936
            vocab_padding_idx: 151646 
            num_key_value_heads: 2
            n_embd: 896
            dim_ffn: 4864
            n_layer: 24
            rope:
                __type__: configs.RoPE_Config
                base: 1000000.0
