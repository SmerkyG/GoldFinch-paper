model:
    __type__: configs.Transformer_Config
    #tmix: qwen2rwkv
    classname: qwen2
    attention_type: rwkv
    ctx_len: 512 # 4096
    vocab_size: 151936
    vocab_padding_idx: 151646 
    num_key_value_heads: 2
    head_size: 128
    n_embd: 1536
    dim_ffn: 8960
    n_layer: 28
    rope:
        __type__: configs.RoPE_Config
        base: 1000000.0

train:
  teacher:
    path: out/qwen2-1B5.safetensors
    model:
      __type__: configs.Transformer_Config
      #tmix: qwen2
      classname: qwen2
      ctx_len: 512 # 4096
      vocab_size: 151936
      vocab_padding_idx: 151646 
      num_key_value_heads: 2
      head_size: 128
      n_embd: 1536
      dim_ffn: 8960
      n_layer: 28
      rope:
          __type__: configs.RoPE_Config
          base: 1000000.0
