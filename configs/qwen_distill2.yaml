model:
    __type__: configs.Transformer_Config
    tmix: qwen2rwkv
    classname: qwen2
    attention_type: rwkv
    ctx_len: 512 # 4096
    vocab_size: 151936
    vocab_padding_idx: 151646 
    num_key_value_heads: 2
    n_embd: 896
    dim_ffn: 4864
    n_layer: 24
    rope:
        __type__: configs.RoPE_Config
        base: 1000000.0

train:
  proj_name: qwen2-0B5
  proj_suffix: 2
  train_stage: 3
  #load_model: out/qwen2-0B5.safetensors
  #load_partial: 1
  load_model: out/qwen2-0B5-1/rwkv-final.pth
  load_partial: 0

  data_type: "binidx" 
  data_file: "data/dclm-10B" #"data/minipile_qwen2" 
  my_exit_tokens: 40000000 # 1491785955
  magic_prime: 2929793 # 2913641 # 364193

  lr_init: 3e-4
  lr_final: 1e-8
  warmup_steps: 500
  beta1: 0.9 
  beta2: 0.95
  adam_eps: 1e-8 
  weight_decay: 0.1 

  devices: 4
  num_nodes: 1
  micro_bsz: 8
  accumulate_grad_batches: 1 # total bsz 32
  strategy: deepspeed_stage_1
  grad_cp: 0
  ds_bucket_mb: 2 # set to 2 for consumer GPUs, set to 200 for A100 / H100 (affects speed & vram usage)
  
  teacher:
    path: out/qwen2-0B5.safetensors
    attention_distillation_stage: 2
    model:
      __type__: configs.Transformer_Config
      #tmix: qwen2
      classname: qwen2
      ctx_len: 512 # 4096
      vocab_size: 151936
      vocab_padding_idx: 151646 
      num_key_value_heads: 2
      n_embd: 896
      dim_ffn: 4864
      n_layer: 24
      rope:
          __type__: configs.RoPE_Config
          base: 1000000.0
